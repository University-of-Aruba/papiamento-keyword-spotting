{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5080f877",
   "metadata": {},
   "source": [
    "# Example Code for \"Speech-to-text model for keyword spotting applications in the Papiamento language within a healthcare environment\"\n",
    "\n",
    "_This Jupyter Notebook provides a PyTorch implementation of the work carried out in the SISSTEM Bachelor thesis **\"Speech-to-text model for keyword spotting applications in the Papiamento language within a healthcare environment\"** at the University of Aruba._\n",
    "\n",
    "It provides a demonstration of STT keyword spotting for the Papiamento language and contains a reproducible workflow for dataset loading, preprocessing, model training, and evaluation.\n",
    "\n",
    "## License and Citation\n",
    "This notebook is licensed under the [Apache 2.0 License](https://opensource.org/licenses/Apache-2.0). If you use this code, please cite it as follows:\n",
    "\n",
    "```bibtex\n",
    "@thesis{rajnherc2025papiamento,\n",
    "  author       = {Rajnherc, Joel Riven}, \n",
    "  type         = {Bachelor's Thesis},\n",
    "  title        = {Speech-to-text model for keyword spotting applications in the Papiamento language within a healthcare environment},\n",
    "  school       = {University of Aruba},\n",
    "  year         = {2025},\n",
    "  month        = {April}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb402292",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# First we check that the necessary version of the librarires, from the requirements.txt file, are installed.\n",
    "# If not, we re-install them using pip.\n",
    "\n",
    "\n",
    "def check_package_versions(requirements_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Check if the required packages are installed and match the versions specified in requirements.txt.\n",
    "    If not, install them using pip.\n",
    "\n",
    "    Args:\n",
    "        requirements_file (str): Path to the requirements.txt file containing package specifications.\n",
    "    \"\"\"\n",
    "    # Load the contents of requirements.txt\n",
    "    # Ensure that the project root directory (parent of the notebook) is the current working directory\n",
    "    project_root = os.path.dirname(os.getcwd())\n",
    "    requirements_file = os.path.join(project_root, requirements_file)\n",
    "    if not os.path.exists(requirements_file):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Requirements file '{requirements_file}' not found.\")\n",
    "\n",
    "    required_package_versions = []\n",
    "    with open(requirements_file, 'r') as file:\n",
    "        required_packages = file.read().splitlines()\n",
    "        # Skip empty lines and comments\n",
    "        required_packages = [pkg.strip(\n",
    "        ) for pkg in required_packages if pkg.strip() and not pkg.startswith('#')]\n",
    "        required_package_versions = [\n",
    "            pkg.split('==') for pkg in required_packages if '==' in pkg]\n",
    "\n",
    "    # Use importlib.metadata to check installed package versions\n",
    "    import importlib.metadata\n",
    "\n",
    "    def get_installed_version(package):\n",
    "        try:\n",
    "            return importlib.metadata.version(package)\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            return None\n",
    "\n",
    "    for package, version in required_package_versions:\n",
    "        try:\n",
    "            installed_version = get_installed_version(package)\n",
    "            if installed_version != version:\n",
    "                print(\n",
    "                    f\"Package '{package}' version mismatch: required {version}, found {installed_version}. Reinstalling...\")\n",
    "                os.system(\n",
    "                    f\"{sys.executable} -m pip install {package}=={version}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Package '{package}' is already installed with the required version {version}.\")\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            print(\n",
    "                f\"Package '{package}' not found. Installing version {version}...\")\n",
    "            os.system(f\"{sys.executable} -m pip install {package}=={version}\")\n",
    "\n",
    "\n",
    "check_package_versions('requirements.txt')\n",
    "\n",
    "\n",
    "def verify_num_workers_per_platform(num_workers: int = 2) -> int:\n",
    "    \"\"\"\n",
    "    Verify the number of workers based on the platform.\n",
    "    Used by DataLoader to set the no. workers when loading data.\n",
    "\n",
    "    Args:\n",
    "        num_workers (int): The number of workers to set.\n",
    "\n",
    "    Returns:\n",
    "        int: The verified number of workers.\n",
    "    \"\"\"\n",
    "    import platform\n",
    "    # Detect platform and set multiprocessing start method if needed\n",
    "    if platform.system() == \"Darwin\":  # macOS\n",
    "        import multiprocessing\n",
    "        try:\n",
    "            multiprocessing.set_start_method(\"fork\")\n",
    "            print(\"Multiprocessing start method set to 'fork' on macOS.\")\n",
    "            print(f\"Variable NUM_WORKERS set to 2.\")\n",
    "            return 2\n",
    "        except RuntimeError as e:\n",
    "            print(\n",
    "                f\"Multiprocessing start method already set or not supported: {e}\")\n",
    "            print(f\"Variable NUM_WORKERS set to 0.\")\n",
    "            return 0\n",
    "    elif platform.system() == \"Linux\":\n",
    "        print(f\"Variable NUM_WORKERS set to 2.\")\n",
    "        return 2\n",
    "    else:  # Windows or unknown\n",
    "        print(f\"Variable NUM_WORKERS set to 0.\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "NUM_WORKERS = verify_num_workers_per_platform(num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e5742",
   "metadata": {},
   "source": [
    "# Dataset Download and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file from the provided URL and extract it to the specified directory\n",
    "def download_and_extract_dataset(\n",
    "    url: str,\n",
    "    compressed_file_name: str,\n",
    "    extract_to: str,\n",
    "    root_data_folder: str,\n",
    "    overwrite=False,\n",
    "    extract_only=True,\n",
    ") -> str:\n",
    "    \"\"\"Downloads a zip file from the given URL and extracts it to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the zip file to download.\n",
    "        compressed_file_name (str): The name of the zip file to be saved locally.\n",
    "        extract_to (str): The directory where the contents should be extracted.\n",
    "        root_data_folder (str): The root folder inside the zip file to extract.\n",
    "        overwrite (bool): If True, will overwrite the existing files in the directory.\n",
    "        extract_only (bool): If True, will only extract the contents if the directory is empty\n",
    "        or does not exist.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the extracted dataset root directory.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    from io import BytesIO\n",
    "\n",
    "    # Check that there are a total of 16766 files in 7 folders (classes)\n",
    "    def verify_dataset_files(directory: str) -> bool:\n",
    "        exp_file_count = 16766\n",
    "        exp_folder_count = 7\n",
    "        total_files = 0\n",
    "        total_dirs = 0\n",
    "        for _, dirs, files in os.walk(directory):\n",
    "            # Exclude files that are not .npz\n",
    "            for file in files:\n",
    "                if file.endswith(\".npz\"):\n",
    "                    total_files += 1\n",
    "            total_dirs += len(dirs)\n",
    "        intact = total_files == exp_file_count and total_dirs == exp_folder_count\n",
    "        print(f\"Expected {exp_file_count} files and {exp_folder_count} folders.\")\n",
    "        print(f\"Found {total_files} files and {total_dirs} folders.\")\n",
    "        return intact\n",
    "\n",
    "    zip_file_name = compressed_file_name\n",
    "    zip_file_path = os.path.join(os.getcwd(), zip_file_name)\n",
    "    # Ensure the extract_to directory exists\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.makedirs(extract_to)\n",
    "\n",
    "    # If extract_only is False, we will always download the dataset\n",
    "    if not extract_only:\n",
    "        # Show download progress and save to zip_file_name\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            if response.status_code == 200:\n",
    "                total_length = int(response.headers.get(\"content-length\", 0))\n",
    "                chunk_size = 8192\n",
    "                downloaded = 0\n",
    "                with open(zip_file_name, \"wb\") as f:\n",
    "                    print(\"Downloading dataset...\")\n",
    "                    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "                            done = (\n",
    "                                int(50 * downloaded / total_length)\n",
    "                                if total_length\n",
    "                                else 0\n",
    "                            )\n",
    "                            sys.stdout.write(\n",
    "                                \"\\r[{}{}] {:.2f}%\".format(\n",
    "                                    \"=\" * done,\n",
    "                                    \" \" * (50 - done),\n",
    "                                    (\n",
    "                                        100 * downloaded / total_length\n",
    "                                        if total_length\n",
    "                                        else 0\n",
    "                                    ),\n",
    "                                )\n",
    "                            )\n",
    "                            sys.stdout.flush()\n",
    "                    print()  # Newline after progress bar\n",
    "                print(f\"Download complete. Saved to {zip_file_name}.\")\n",
    "                with ZipFile(zip_file_path, \"r\") as zip_file:\n",
    "                    print(f\"Extracting zip file to {extract_to}...\")\n",
    "                    zip_file.extractall(path=extract_to)\n",
    "                print(\"Extraction complete.\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Failed to download dataset. Status code: {response.status_code}\"\n",
    "                )\n",
    "\n",
    "    # If overwrite is True, we extract the existing zip file\n",
    "    if overwrite:\n",
    "        # Check if the zip file already exists\n",
    "        if os.path.exists(zip_file_path):\n",
    "            try:\n",
    "                print(f\"Using existing zip file: {zip_file_path}\")\n",
    "                with ZipFile(zip_file_path, \"r\") as zip_file:\n",
    "                    print(f\"Extracting zip file to {extract_to}...\")\n",
    "                    zip_file.extractall(path=extract_to)\n",
    "                print(\"Extraction complete.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract zip file: {e}\")\n",
    "                print(\n",
    "                    \"The zip file may be corrupted. Please delete it and re-download.\"\n",
    "                )\n",
    "                return \"\"\n",
    "        else:\n",
    "            print(\n",
    "                f\"The zip file does not exist at {zip_file_path}. Set extract_only=False to download it.\"\n",
    "            )\n",
    "            return \"\"\n",
    "\n",
    "    # Check if the dataset is already extracted and intact\n",
    "    print(f\"Checking if existing dataset is intact...\")\n",
    "    if verify_dataset_files(os.path.join(extract_to, root_data_folder)):\n",
    "        print(f\"Dataset is intact.\")\n",
    "    else:\n",
    "        raise Exception(\"Dataset verification failed. Please check the dataset.\")\n",
    "\n",
    "    # Return the full dataset path\n",
    "    dataset_path = os.path.join(os.getcwd(), \"data_directory\", \"stft_spectrograms\")\n",
    "    print(f\"Variable DATASET_PATH set to {dataset_path}.\")\n",
    "    return dataset_path\n",
    "\n",
    "\n",
    "DATASET_PATH = download_and_extract_dataset(\n",
    "    url=\"https://zenodo.org/records/15794240/files/papiamento_keyword_spotting_dataset.zip?download=1\",\n",
    "    compressed_file_name=\"papiamento_keyword_spotting_dataset.zip\",\n",
    "    extract_to=\"data_directory\",\n",
    "    root_data_folder=\"stft_spectrograms\",\n",
    "    overwrite=False,  # Set to True to overwrite the existing files when extracting\n",
    "    extract_only=False,  # Set to False to download the dataset if it does not exist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2752512",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ef2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Custom data loader for npz files\n",
    "def npz_loader(path: str) -> np.ndarray:\n",
    "    with np.load(path) as data:\n",
    "        return data[\"arr_0\"]\n",
    "\n",
    "\n",
    "# Specify the seed worker\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def to_tensor_float(x):\n",
    "    return torch.from_numpy(x).float()\n",
    "\n",
    "\n",
    "def add_channel_dim(x):\n",
    "    return x.unsqueeze(0)\n",
    "\n",
    "\n",
    "# Custom dataset class to apply normalization transform\n",
    "# Necessary because DatasetFolder doesn't support transforms on subsets\n",
    "class ApplyNormalizeTransform(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "        return self.transform(data), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def dataset_loader(dataset_path: str, num_workers: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Function to load the dataset using DatasetFolder from torchvision.\n",
    "    It applies necessary transformations and splits the dataset into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the dataset.\n",
    "        num_workers (int): The number of workers for DataLoader.\n",
    "    \"\"\"\n",
    "    # Use imported functions from src/data_utils.py\n",
    "    dataset = DatasetFolder(\n",
    "        root=DATASET_PATH,\n",
    "        loader=npz_loader,\n",
    "        extensions=(\".npz\",),\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.Lambda(to_tensor_float),\n",
    "                transforms.Lambda(add_channel_dim),  # To add a channel dim\n",
    "                transforms.Resize((32, 32)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    val_size = int(0.2 * dataset_size)\n",
    "    train_size = dataset_size - val_size\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size], generator=generator\n",
    "    )\n",
    "\n",
    "    # Get the mean and standard deviation from `train_dataset`, for normalization.\n",
    "    # We use a temporary DataLoader to iterate through the dataset to calculate mean and std.\n",
    "    temp_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    # Calculate mean and std\n",
    "    mean_sum = 0.0\n",
    "    std_sum = 0.0\n",
    "    num_batches = 0\n",
    "    for data, _ in temp_loader:\n",
    "        mean_sum += data.mean()\n",
    "        std_sum += data.std()\n",
    "        num_batches += 1\n",
    "\n",
    "    mean = mean_sum / num_batches\n",
    "    std = std_sum / num_batches\n",
    "\n",
    "    # Now, apply the normalization transform to the datasets\n",
    "    normalize_transform = transforms.Normalize(mean=[mean], std=[std])\n",
    "\n",
    "    train_dataset = ApplyNormalizeTransform(train_dataset, normalize_transform)\n",
    "    val_dataset = ApplyNormalizeTransform(val_dataset, normalize_transform)\n",
    "\n",
    "    # This is used to split validation into val and test subsets\n",
    "    num_shards = 2\n",
    "\n",
    "    # Get all indices in the validation set\n",
    "    all_indices = list(range(len(val_dataset)))\n",
    "\n",
    "    # Test set: shard index 0\n",
    "    test_indices = all_indices[0::num_shards]\n",
    "    test_dataset = Subset(val_dataset, test_indices)\n",
    "    # New validation set: shard index 1\n",
    "    val_indices = all_indices[1::num_shards]\n",
    "    val_dataset = Subset(val_dataset, val_indices)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    class_names = dataset.classes\n",
    "    print(\"Class names:\", class_names)\n",
    "\n",
    "    return mean, std, train_loader, val_loader, test_loader, class_names\n",
    "\n",
    "\n",
    "# Load the subsets, the class names, the mean and std of the training set for normalization\n",
    "mean, std, train_loader, val_loader, test_loader, class_names = dataset_loader(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a6808d",
   "metadata": {},
   "source": [
    "# Dataset Inspection and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2607a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_minibatched_sample_shape(loaded_subset: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Inspect the shape of one sample from the loaded subset.\n",
    "\n",
    "    Args:\n",
    "        loaded_subset (DataLoader): The DataLoader subset.\n",
    "    \"\"\"\n",
    "    one_sample, one_label = next(iter(loaded_subset))\n",
    "    print(f\"Shape of one mini-batched sample: {one_sample.shape}\")\n",
    "    print(f\"Shape of one mini-batched label: {one_label.shape}\")\n",
    "\n",
    "\n",
    "# Inspect the shape of one sample from a training set minibatch\n",
    "inspect_minibatched_sample_shape(train_loader)\n",
    "\n",
    "\n",
    "def fetch_audio_and_spectrogram_tensor(file_path: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Fetch the audio signal and its spectrogram tensor from a .npz file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .npz file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the spectrogram tensor and the reconstructed waveform.\n",
    "    \"\"\"\n",
    "    spec_data = npz_loader(file_path)\n",
    "    spec_tensor = torch.from_numpy(spec_data).float()\n",
    "    spec_complex = torch.complex(spec_tensor, torch.zeros_like(spec_tensor))\n",
    "    spec_complex = spec_complex.transpose(0, 1)\n",
    "    freq_bins, _ = spec_complex.shape\n",
    "\n",
    "    n_fft = (freq_bins - 1) * 2\n",
    "    hop_length = 128\n",
    "    win_length = n_fft\n",
    "\n",
    "    window = torch.hann_window(win_length, periodic=False)\n",
    "\n",
    "    reconstructed_waveform = torch.istft(\n",
    "        spec_complex,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        window=window,\n",
    "        center=True,\n",
    "        normalized=False,\n",
    "        onesided=True,\n",
    "        length=None,\n",
    "    )\n",
    "\n",
    "    # Pad or truncate with last value if it is not equal to 16000\n",
    "    if reconstructed_waveform.shape[0] < 16000:\n",
    "        reconstructed_waveform = F.pad(\n",
    "            reconstructed_waveform, (0, 16000 -\n",
    "                                     reconstructed_waveform.shape[0])\n",
    "        )\n",
    "    elif reconstructed_waveform.shape[0] > 16000:\n",
    "        reconstructed_waveform = reconstructed_waveform[:16000]\n",
    "\n",
    "    return spec_tensor, reconstructed_waveform\n",
    "\n",
    "\n",
    "def hear_and_visualize_one_of_each_class(dataset_path: str, class_names: list) -> None:\n",
    "    \"\"\"\n",
    "    Hear and visualize one sample from each class from the raw (untransformed) dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): The path to the dataset.\n",
    "        class_names (list): The list of class names.\n",
    "    \"\"\"\n",
    "\n",
    "    spectrograms = []\n",
    "\n",
    "    # Load one npz file from each class folder\n",
    "    for class_name in class_names:\n",
    "        label_folder = class_name\n",
    "        label_dir = os.path.join(dataset_path, label_folder)\n",
    "        random_file = random.choice(os.listdir(label_dir))\n",
    "        file_path = os.path.join(label_dir, random_file)\n",
    "\n",
    "        # Reconstruct the waveform from the spectrogram\n",
    "        spectrogram, reconstructed_waveform = fetch_audio_and_spectrogram_tensor(\n",
    "            file_path\n",
    "        )\n",
    "        spectrograms.append(spectrogram)\n",
    "\n",
    "        print(f\"Class: {class_name}, File: {random_file}\")\n",
    "        display.display(display.Audio(reconstructed_waveform, rate=16000))\n",
    "\n",
    "    # Now we visualize the spectrograms, in one row and n-columns == len(class_names)\n",
    "    fig, axes = plt.subplots(1, len(class_names), figsize=(20, 3))\n",
    "    fig.suptitle(\"One Sample from Each Class\", fontsize=16)\n",
    "    for i, (ax, spec, class_name) in enumerate(zip(axes, spectrograms, class_names)):\n",
    "        # Compute log-scaled spectrogram for better visualization, avoiding log(0)\n",
    "        log_spec = np.log(spec.numpy().T + np.finfo(np.float32).eps)\n",
    "        height = log_spec.shape[0]\n",
    "        width = log_spec.shape[1]\n",
    "        # Time axis: 0 to 1 second, width points\n",
    "        X = np.linspace(0, 1, num=width)\n",
    "        Y = range(height)\n",
    "        ax.pcolormesh(X, Y, log_spec)\n",
    "        ax.set_title(class_name)\n",
    "        ax.set_xlabel(\"Time (s)\")\n",
    "        ax.set_ylabel(\"Frequency Bins\")\n",
    "        # ax.set_aspect('equal', adjustable='box')  # Set 1:1 aspect ratio\n",
    "        ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.75)  # Adjust top to make room for the suptitle\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "hear_and_visualize_one_of_each_class(\n",
    "    dataset_path=DATASET_PATH, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac8ef7",
   "metadata": {},
   "source": [
    "# Model and Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60668ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, in_channels: int, num_labels: int, mean, std):\n",
    "        super().__init__()\n",
    "        # Convert to (1, C, 1, 1) tensors for broadcasting\n",
    "        mean = mean.detach().clone().view(1, -1, 1, 1)\n",
    "        std = std.detach().clone().view(1, -1, 1, 1)\n",
    "        # Register buffers\n",
    "        self.register_buffer('mean', mean)\n",
    "        self.register_buffer('std',  std)\n",
    "        # Convolutional feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Calculate the correct input size for the linear layer\n",
    "            # After the conv layers and max pooling, the spatial dimensions will be reduced.\n",
    "            # For a 32x32 input, after two 3x3 conv layers with padding=0 and one 2x2 max pool,\n",
    "            # the output spatial dimensions will be:\n",
    "            # $M \\;=\\;\\frac{32 - 2\\cdot(3-1)}{2}\\;=\\;\\frac{32 - 4}{2}\\;=\\;14$\n",
    "            # So the input size to the linear layer will be 64 * 14 * 14 = 12544.\n",
    "            nn.Dropout(0.50),\n",
    "            nn.Linear(64 * 14 * 14, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.75),\n",
    "            nn.Linear(256, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We a channel if it's missing (assuming input is Batch x Height x Width)\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        # Normalize using mean and standard deviation\n",
    "        x = (x - self.mean) / self.std\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SpectrogramCNN(in_channels=1, num_labels=7,\n",
    "                       mean=mean, std=std).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "summary(model, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff4858a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += (preds == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)  # type: ignore\n",
    "    epoch_acc = running_corrects / len(train_loader.dataset)  # type: ignore\n",
    "\n",
    "    history['train_loss'].append(epoch_loss)\n",
    "    history['train_acc'].append(epoch_acc)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_corrects += (preds == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_loss / len(val_loader.dataset)  # type: ignore\n",
    "    val_epoch_acc = val_corrects / len(val_loader.dataset)  # type: ignore\n",
    "\n",
    "    history['val_loss'].append(val_epoch_loss)\n",
    "    history['val_acc'].append(val_epoch_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: \"\n",
    "          f\"train_loss={epoch_loss:.4f}, train_acc={epoch_acc:.4f}, \"\n",
    "          f\"val_loss={val_epoch_loss:.4f}, val_acc={val_epoch_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5629e0e",
   "metadata": {},
   "source": [
    "# Model Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db7458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, num_epochs+1)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history['train_loss'], label='train_loss')\n",
    "plt.plot(epochs, history['val_loss'],   label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.ylim(0, max(max(history['train_loss']), max(history['val_loss'])))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, 100 * np.array(history['train_acc']), label='train_acc')\n",
    "plt.plot(epochs, 100 * np.array(history['val_acc']),   label='val_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.legend()\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65246331",
   "metadata": {},
   "source": [
    "# Model Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deceaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "label_names = np.array(class_names)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = outputs.argmax(dim=1).cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "y_pred = torch.cat(all_preds).numpy()\n",
    "y_true = torch.cat(all_labels).numpy()\n",
    "\n",
    "# Print the accuracy between y_true and y_pred\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "print(f\"Number of test samples: {len(y_true)}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"=\"*27)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_mtx = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot raw confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mtx, annot=True, fmt='g',\n",
    "            xticklabels=label_names, yticklabels=label_names)  # type: ignore\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Normalize and plot\n",
    "conf_mtx_norm = conf_mtx.astype(float) / conf_mtx.sum(axis=1, keepdims=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_mtx_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=label_names, yticklabels=label_names)  # type: ignore\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
